http://www.cnblogs.com/pinard/p/7160330.html

1.最开始使用one-hot方式表示词向量。
 缺点：当词汇表非常大时，只有一个位置为1，其余全为0，导致内存灾难。

2.采用Dristributed representation，将每个词映射到短向量中，构成向量空间，从而可以得知词与词之间的距离。（有两种训练方法如下）

3.最原始的实现DNN实现
例：A B C D E F G H I(其中E为特征词，其他为上下文)
CBOW采用的是输入某个特征词的上下文词对应的词向量，输出这个特征词的词向量。
这样我们这个CBOW的例子里，我们的输入是8个词向量，输出是所有词的softmax概率（训练的目标是期望训练样本特定词对应的softmax概率最大），
对应的CBOW神经网络模型输入层有8个神经元，输出层有词汇表大小个神经元

skip_gram采用的根据目标词向量，输出这个词的上下文的词向量
在skip_gram中会有一个神经元词向量输入，输出词汇表大小大小神经元，与这个词组成的softmax概率，从而选取概率最大的前8个，如上例

4.最先优化使用的数据结构是用霍夫曼树来代替隐藏层和输出层的神经元。
为了解决计算所有词的softmax概率
1）首先解决的为输入变为一个词向量（原来为词向量矩阵）---上下文的词取平均
2）为了解决词向量在隐藏层到输出层，计算所有词的softmax，引进层softmax-即霍夫曼树softmax
霍夫曼树，每次抽取权重低的两个叶子节点，生成跟，以此选取权重低的节点，生成跟。。。以此类推

霍夫曼树softmax，根节点为特征词向量，（cbow中是采用将上下文的词向量求和取平均变成一个词向量，skip_gram中直接使用输入的目标词向量即可）
从根节点沿着节点一步步往下走，找到目标向量
沿着霍夫曼树一步步选择，是采用二元逻辑回归，即沿着左子树走还是右子树走，使用sigmoid函数判断每一步输出。
由于是二叉树，所以计算量为logV，越靠近词根为高频词。

5.基于负采样实现
当目标词为生僻词时，需要向下走的时间变长。
负采样的方法：得到neg个负例，[（词频）^3/4]/【所有词频的^3/4累加】---越高频的词越容易抽中
通过一个正例和n个负采样，使用二元逻辑回归


---------
根据具体的任务，选取相似的语料训练，语料越大越好。
语料小（小于1亿的词）的时候使用skip_gram，语料大的时候，使用CBOW。
skip_gram是利用目标词来找上下文，每个目标词都要做k次预测和调整，而cbow则每次只会做一个预测，再进行调整。
所以skip_gram（k*v(vocab size)）训练时间比cbow(v)要长。
skip_gram处理生僻词更好，因为当一个生僻词出现的时候，每组词（输入-输出）单独训练，cbow反作用回给输入，而所有输入的词取个平均。
